# ğŸ“„ Pipeline documentaire â€“ Nettoyage â€¢ DÃ©doublonnage â€¢ Conversion â€¢ Classification â€¢ Export Markdown â€¢ Dataset LLM

## ğŸ§© SchÃ©ma global du pipeline (ASCII)

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚    raw/ (brut)     â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ 1) clean_extension.py          â”‚
            â”‚ - Filtrage extensions          â”‚
            â”‚ - Suffixes antiâ€‘collision      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ clean_extension/                    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ 2) dedupe.py                   â”‚
            â”‚ - RÃ¨gles DOC/DOCX/PDF          â”‚
            â”‚ - SÃ©lection fichier le + rÃ©centâ”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚       dedupe/          â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 3) convert_to_docx.py                  â”‚
        â”‚ - DOC â†’ DOCX (LibreOffice)             â”‚
        â”‚ - PDF â†’ DOCX (pdf2docx)                â”‚
        â”‚ - Copie des DOCX                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   docx/   â”‚
                   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ 4) classify_docx.py                        â”‚
       â”‚ - Analyse 1Ã¨re page                        â”‚
       â”‚ - DÃ©tection EDB / NDC / AUTRES             â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ classified_docx/                              â”‚
         â”‚   â”œâ”€â”€ edb/                                   â”‚
         â”‚   â”œâ”€â”€ ndc/                                   â”‚
         â”‚   â””â”€â”€ autres/                                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ 5) convert_classified_to_md.py                     â”‚
      â”‚ - DOCX â†’ Markdown                                  â”‚
      â”‚ - Export EDB & NDC                                 â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ markdown/                                 â”‚
         â”‚   â”œâ”€â”€ edb/                                â”‚
         â”‚   â””â”€â”€ ndc/                                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ 6) build_dataset_jsonl.py                          â”‚
      â”‚ - Appariement EDB â†” NDC                            â”‚
      â”‚ - Export JSONL pour fine-tuning                    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ train_dataset.jsonl                       â”‚
         â”‚ val_dataset.jsonl                         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ“˜ Description gÃ©nÃ©rale

Ce dÃ©pÃ´t contient un pipeline complet permettant de transformer un lot de documents bruts en un ensemble :

- propre
- dÃ©doublonnÃ©
- homogÃ¨ne
- converti au format DOCX
- classÃ© automatiquement (NDC / EDB / AUTRES)
- exportÃ© en Markdown
- prÃªt pour fine-tuning LLM (dataset JSONL)

Il repose sur **sept scripts Python** :

**Pipeline principal (Ã©tapes 1-5) :**
1. `clean_extension.py`
2. `dedupe.py`
3. `convert_to_docx.py`
4. `classify_docx.py`
5. `convert_classified_to_md.py`

**Scripts complÃ©mentaires :**
6. `extract_docx_to_markdown.py` â€” Extraction DOCX â†’ Markdown (via Excel de mapping)
7. `build_dataset_jsonl.py` â€” Constitution dataset JSONL pour fine-tuning  

---

# ğŸš€ ExÃ©cution depuis un Pod JupyterLab (template *scribe*)

## âœ”ï¸ Instructions exactes

### **1) CrÃ©er un Pod**

- Template : **scribe**
- **Sans GPU**
- Ouvrir JupyterLab
- Ouvrir un Terminal

### **2) Installer lâ€™environnement**

```bash
bash
git clone https://github.com/TeamCLP/datas.git /home/datas && source /home/datas/install.sh
```

Le script `install.sh` configure automatiquement :

- Proxy  
- LibreOffice  
- Miniconda + Python 3.13  
- Environnement `pipeline`  
- Installation du `requirements.txt`  
- Activation du venv  
- Positionnement dans `/home/datas`

### **3) DÃ©poser les donnÃ©es sources**

Placer `raw_datas.tar` ici :

```
/home/datas
```

Puis extraire :

```bash
tar -xvf raw_datas.tar -C raw/
```

### **4) Lancer le pipeline complet**

```bash
python clean_extension.py
python dedupe.py
python convert_to_docx.py
python classify_docx.py
python convert_classified_to_md.py
```

---

# ğŸ§± Architecture finale

AprÃ¨s exÃ©cution :

```
datas/
â”œâ”€â”€ raw/
â”œâ”€â”€ clean_extension/
â”œâ”€â”€ dedupe/
â”œâ”€â”€ docx/
â”œâ”€â”€ classified_docx/
â”‚   â”œâ”€â”€ edb/
â”‚   â”œâ”€â”€ ndc/
â”‚   â””â”€â”€ autres/
â”œâ”€â”€ markdown/
â”‚   â”œâ”€â”€ edb/
â”‚   â””â”€â”€ ndc/
â”œâ”€â”€ clean_extension.py
â”œâ”€â”€ dedupe.py
â”œâ”€â”€ convert_to_docx.py
â”œâ”€â”€ classify_docx.py
â”œâ”€â”€ convert_classified_to_md.py
â”œâ”€â”€ extract_docx_to_markdown.py
â”œâ”€â”€ build_dataset_jsonl.py
â”œâ”€â”€ train_dataset.jsonl
â”œâ”€â”€ val_dataset.jsonl
â””â”€â”€ README.md
```

---

# âš™ï¸ 1. Ã‰tape 1 â€” Nettoyage des extensions  
**Script : `clean_extension.py`**

### RÃ´le

- Parcourt `raw/`
- Ne conserve que : `.pdf`, `.doc`, `.docx`
- Ajoute un suffixe `_YYYYMMDD_HHMMSS` en cas de collision
- Produit : `inventaire_raw.xlsx`
- Remplit : `clean_extension/`

### ExÃ©cution

```bash
python clean_extension.py
```

---

# ğŸ§¹ 2. Ã‰tape 2 â€” DÃ©doublonnage intelligent  
**Script : `dedupe.py`**

### RÃ¨gles mÃ©tier

| Cas | Conserver |
|-----|-----------|
| `.docx` prÃ©sent | `.docx` le plus rÃ©cent |
| `.doc` sans `.docx` | `.doc` le plus rÃ©cent |
| seulement PDF | PDF le plus rÃ©cent |

### Sorties

- rÃ©pertoire : `dedupe/`
- rapport : `dedupe_report.xlsx`

### ExÃ©cution

```bash
python dedupe.py
```

---

# ğŸ” 3. Ã‰tape 3 â€” Conversion DOCâ†’DOCX & PDFâ†’DOCX  
**Script : `convert_to_docx.py`**

### RÃ´le

- Conversion `.doc` via LibreOffice  
- Conversion `.pdf` via `pdf2docx`  
- Copie des `.docx` existants  
- Output : `docx/`
- Rapport : `convert_report.xlsx`

### Options

- `--on-exists skip` (dÃ©faut)  
- `--on-exists overwrite`  
- `--on-exists suffix`  

### ExÃ©cution

```bash
python convert_to_docx.py
```

---

# ğŸ” 4. Ã‰tape 4 â€” Classification des DOCX
**Script : `classify_docx.py`**

### RÃ´le

Analyse de la **premiÃ¨re page** et du **nom de fichier** selon cet ordre :

1. **NDC** si code dÃ©tectÃ© en 1Ã¨re page
2. **EDB** si le nom contient "edb"
3. **EDB** si le nom contient "expression de besoin(s)"
4. **EDB** si le nom contient "eb" ET pas de code NDC en 1Ã¨re page
5. **NDC** si code dÃ©tectÃ© dans le nom du fichier
6. **EDB** si la 1Ã¨re page contient "expression de besoin(s)"
7. **AUTRES** sinon

### Motif NDC

Pattern reconnu : `CLIENT` + `ANNÃ‰E` + `CODE`

- **CLIENT** : `CAPS` ou `AVEM` (tolÃ©rance aux espaces internes)
- **ANNÃ‰E** : 4 caractÃ¨res alphanumÃ©riques (ex: `2024`, `A2B3`)
- **CODE** : alphanumÃ©rique avec tirets/underscores

Exemples : `CAPS_2024_001`, `AVEM2023-42_PF`, `C A P S_A1B2_123`

### Sorties

```
classified_docx/
    edb/
    ndc/
    autres/
```

### Rapport

```
classify_report.xlsx  (dans le dossier racine datas/)
```

### ExÃ©cution

```bash
python classify_docx.py
```

---

# âœï¸ 5. Ã‰tape 5 â€” Export Markdown  
**Script : `convert_classified_to_md.py`**

### RÃ´le

- Convertit en Markdown tous les fichiers de :
  - `classified_docx/ndc/`
  - `classified_docx/edb/`

- DÃ©pose les `.md` dans :
  - `markdown/ndc/`
  - `markdown/edb/`

### ExÃ©cution

```bash
python convert_classified_to_md.py
```

---

# ğŸ“¤ 6. Extraction DOCX â†’ Markdown (alternative)
**Script : `extract_docx_to_markdown.py`**

### RÃ´le

Script alternatif d'extraction basÃ© sur un fichier Excel de mapping :

- Lit un fichier Excel contenant les chemins des EDB et NDC
- Convertit les DOCX en Markdown via **Mammoth** (meilleure qualitÃ©)
- Supprime automatiquement : page de garde, table des matiÃ¨res, prÃ©ambule
- PrÃ©serve : titres, paragraphes, listes, tableaux

### Configuration

Modifier les constantes en dÃ©but de fichier :

```python
EXCEL_NAME = "couverture_EDB_NDC_par_RITM.xlsx"
COL_EDB = 5  # Colonne F
COL_NDC = 6  # Colonne G
EXCEL_FILTERS = [(3, "OUI")]  # Filtre colonne D = "OUI"
```

### Sorties

```
dataset_markdown/
â”œâ”€â”€ edb/
â”œâ”€â”€ ndc/
â”œâ”€â”€ _logs/
â””â”€â”€ conversion_report.csv
```

### ExÃ©cution

```bash
python extract_docx_to_markdown.py
```

---

# ğŸ¤– 7. Constitution du dataset JSONL
**Script : `build_dataset_jsonl.py`**

### RÃ´le

Construit un dataset JSONL pour fine-tuning LLM (Mistral Instruct) :

- Apparie les fichiers EDB et NDC par rÃ©fÃ©rence (ex: `CAGIPRITM123456`)
- GÃ¨re les cas multi-versions (plusieurs EDB/NDC pour une mÃªme rÃ©fÃ©rence)
- Split train/val configurable (90/10 par dÃ©faut)
- Format compatible Mistral Instruct / ChatML / Alpaca

### StratÃ©gies de mapping multi-fichiers

| StratÃ©gie | Description |
|-----------|-------------|
| `version_match` | Apparie par version dÃ©tectÃ©e (v1â†”v1, Etudeâ†”Etude) |
| `all_combinations` | CrÃ©e toutes les combinaisons EDBÃ—NDC |
| `latest_only` | Utilise uniquement la version la plus rÃ©cente |
| `first_only` | Utilise le premier fichier trouvÃ© |

### ExÃ©cution

```bash
# ExÃ©cution standard
python build_dataset_jsonl.py

# Avec rapport dÃ©taillÃ©
python build_dataset_jsonl.py --report

# Simulation sans Ã©criture
python build_dataset_jsonl.py --dry-run --report

# Options avancÃ©es
python build_dataset_jsonl.py --strategy all_combinations --train_ratio 0.8
```

### Sorties

- `train_dataset.jsonl` â€” Dataset d'entraÃ®nement
- `val_dataset.jsonl` â€” Dataset de validation

---

# ğŸ§­ 8. Pipeline complet (ordre recommandÃ©)

```bash
# Pipeline principal (traitement des documents bruts)
python clean_extension.py
python dedupe.py
python convert_to_docx.py
python classify_docx.py
python convert_classified_to_md.py

# Constitution du dataset LLM (aprÃ¨s le pipeline principal)
python build_dataset_jsonl.py --report
```

---

# ğŸ“Š 9. Fichiers Excel/CSV gÃ©nÃ©rÃ©s

| Ã‰tape | Fichier | Emplacement | Contenu |
|-------|---------|-------------|---------|
| Nettoyage | `inventaire_raw.xlsx` | `datas/` | inventaire et actions appliquÃ©es |
| DÃ©doublonnage | `dedupe_report.xlsx` | `datas/` | rÃ¨gles, dÃ©cisions, justification |
| Conversion | `convert_report.xlsx` | `datas/` | conversion/copied, logs |
| Classification | `classify_report.xlsx` | `datas/` | EDB / NDC / AUTRES + destination |
| Extraction | `conversion_report.csv` | `dataset_markdown/` | statut extraction DOCX â†’ MD |

---

# â­ 10. Bonnes pratiques

- Toujours suivre le pipeline dans lâ€™ordre  
- Ne jamais modifier manuellement les dossiers intermÃ©diaires  
- Conserver `--on-exists skip` sauf besoin explicite  
- Utiliser les rapports Excel pour audit et contrÃ´le  

---

# ğŸ§© 11. RÃ©sultat attendu

Ã€ la fin du pipeline :

- Fichiers nettoyÃ©s
- Doublons supprimÃ©s
- Corpus converti Ã  100% en `.docx`
- Documents automatiquement classÃ©s
- Export Markdown propre et structurÃ©
- Dataset JSONL prÃªt pour fine-tuning
- TraÃ§abilitÃ© complÃ¨te

Le pipeline produit un corpus documentaire propre, homogÃ¨ne et un dataset directement exploitable pour le fine-tuning de LLM.
